# CodeAssistantAnalysis
## Errors in executing code are common occurrences among programmers. When a code is written incorrectly, an error occurs that needs to be fixed by the programmer. Modifying example code online or searching for solutions through forums or blogs is one of the ways often used to solve this problem. Currently, platforms such as ChatGPT, Google Search, and Stack Overflow are the primary choices for programmers to search for answers and solutions to the errors they encounter. However, there is a lack of research on the code competency provided by each assistant and the code similarity to the solutions. This study aims to compare the effectiveness of each assistant in providing solutions that are easily understood by programmers, considering the similarity and competence level of the code generated by code assistants. Two methods of measuring and analyzing code, namely similarity measurement using NCD Search and code competency measurement using PyCEFR are used. In addition, this research evaluates the use of keywords on code assistants and how they impact code executions and tab switching. As the result, it suggests that ChatGPT is a more effective and efficient coding assistant than Google Search and Stack Overflow in reducing coding errors and generating higher proficiency and dissimilarity of code, requiring less time and effort from programmers and using less tab switching and program execution. ChatGPT also provides unique and diverse code suggestions, outperforming the other two in this aspect. This research can be beneficial for software engineers, developers, and researchers interested in the fields of code search, code similarity, and code competence.
## Keywords: code competency, code similarity, google search, stack overflow, chatgpt
